{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0keM7jrT044",
        "outputId": "6133849d-9efc-44b3-e712-ebe2f6b5d62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TensorBoardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7-o-1wcT2hX",
        "outputId": "c401daf9-b4d0-414c-be3e-c996362b5d3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting TensorBoardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from TensorBoardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from TensorBoardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->TensorBoardX) (1.15.0)\n",
            "Installing collected packages: TensorBoardX\n",
            "Successfully installed TensorBoardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk5ek1BmT4IS",
        "outputId": "c32e9c73-c6f3-40ac-be76-79edd5833c8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import yaml\n",
        "import sklearn\n",
        "import tensorboardX"
      ],
      "metadata": {
        "id": "Pn4LSYctT6Ia"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/BDAProject/imbalanced-semi-self-master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MADtOxe1T78y",
        "outputId": "a4c1988e-5b3e-47f8-8ba2-efa4eaf95183"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9vNWr_mUBhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python pretrain_moco.py --dataset imagenet --data '/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/data/ImageNet'"
      ],
      "metadata": {
        "id": "bCLE_oqXUBeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a374712b-741a-4ed8-abe3-344f2a711190"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: 0 for training\n",
            "=> creating model 'resnet50'\n",
            "MoCo(\n",
            "  (encoder_q): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (encoder_k): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "===> Training data length 38267\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Traceback (most recent call last):\n",
            "  File \"pretrain_moco.py\", line 412, in <module>\n",
            "    main()\n",
            "  File \"pretrain_moco.py\", line 130, in main\n",
            "    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\n",
            "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\n",
            "    while not context.join():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 146, in join\n",
            "    signal_name=name\n",
            "torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL\n",
            "/usr/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 104 leaked semaphores to clean up at shutdown\n",
            "  len(cache))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m imagenet_inat.main --cfg '/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/imagenet_inat/config/ImageNet_LT/feat_balance.yaml' --model_dir '/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/data/imagenet/moco_ckpt_0200.pth.tar'"
      ],
      "metadata": {
        "id": "bitvz6iYUCPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb956d97-cd11-4b18-9012-87268301a890"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: ./data\n",
            "{'coslr': True,\n",
            " 'criterions': {'PerformanceLoss': {'def_file': './imagenet_inat/loss/SoftmaxLoss.py',\n",
            "                                    'loss_params': {},\n",
            "                                    'optim_params': None,\n",
            "                                    'weight': 1.0}},\n",
            " 'endlr': 0.0,\n",
            " 'last': False,\n",
            " 'memory': {'centroids': False, 'init_centroids': False},\n",
            " 'model_dir': '/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/data/imagenet/moco_ckpt_0200.pth.tar',\n",
            " 'networks': {'classifier': {'def_file': './imagenet_inat/models/DotProductClassifier.py',\n",
            "                             'optim_params': {'lr': 0.2,\n",
            "                                              'momentum': 0.9,\n",
            "                                              'weight_decay': 0.0005},\n",
            "                             'params': {'dataset': 'ImageNet_LT',\n",
            "                                        'feat_dim': 2048,\n",
            "                                        'log_dir': './data',\n",
            "                                        'num_classes': 1000,\n",
            "                                        'stage1_weights': False}},\n",
            "              'feat_model': {'def_file': './imagenet_inat/models/ResNet50Feature.py',\n",
            "                             'fix': False,\n",
            "                             'optim_params': {'lr': 0.2,\n",
            "                                              'momentum': 0.9,\n",
            "                                              'weight_decay': 0.0005},\n",
            "                             'params': {'dataset': 'ImageNet_LT',\n",
            "                                        'dropout': None,\n",
            "                                        'log_dir': './data',\n",
            "                                        'stage1_weights': False,\n",
            "                                        'use_fc': False,\n",
            "                                        'use_selfatt': False}}},\n",
            " 'shuffle': True,\n",
            " 'training_opt': {'backbone': 'resnet50',\n",
            "                  'batch_size': 128,\n",
            "                  'dataset': 'ImageNet_LT',\n",
            "                  'display_step': 10,\n",
            "                  'feature_dim': 2048,\n",
            "                  'log_dir': './data',\n",
            "                  'log_root': './data',\n",
            "                  'num_classes': 1000,\n",
            "                  'num_epochs': 90,\n",
            "                  'num_workers': 12,\n",
            "                  'open_threshold': 0.1,\n",
            "                  'sampler': {'def_file': './imagenet_inat/data/ClassAwareSampler.py',\n",
            "                              'num_samples_cls': 4,\n",
            "                              'type': 'ClassAwareSampler'},\n",
            "                  'scheduler_params': {'gamma': 0.1, 'step_size': 30},\n",
            "                  'stage': 'resnet50_balance_e90_ssp_moco',\n",
            "                  'sub_dir': 'models'}}\n",
            "Loading data from ./imagenet_inat/data/ImageNet_LT/ImageNet_LT_train.txt\n",
            "Use data transformation: Compose(\n",
            "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    ColorJitter(brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=None)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n",
            "38267\n",
            "Using sampler:  <class '.ClassAwareSampler'>\n",
            "Sampler parameters:  {'num_samples_cls': 4}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Loading data from ./imagenet_inat/data/ImageNet_LT/ImageNet_LT_train.txt\n",
            "Use data transformation: Compose(\n",
            "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n",
            "38267\n",
            "No sampler.\n",
            "Shuffle is True.\n",
            "Loading data from ./imagenet_inat/data/ImageNet_LT/ImageNet_LT_val.txt\n",
            "Use data transformation: Compose(\n",
            "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n",
            "16000\n",
            "No sampler.\n",
            "Shuffle is True.\n",
            "Loading data from ./imagenet_inat/data/ImageNet_LT/ImageNet_LT_test.txt\n",
            "Use data transformation: Compose(\n",
            "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n",
            "16000\n",
            "No sampler.\n",
            "Shuffle is True.\n",
            "Using 1 GPUs.\n",
            "Loading Dot Product Classifier.\n",
            "Random initialized classifier weights.\n",
            "Loading Scratch ResNet 50 Feature Model.\n",
            "No Pretrained Weights For Feature Model.\n",
            "Validation on the best model.\n",
            "Loading model from /content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/data/imagenet/moco_ckpt_0200.pth.tar\n",
            "Skipping classifier initialization\n",
            "['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.bn1.running_mean', 'module.bn1.running_var', 'module.bn1.num_batches_tracked', 'module.layer1.0.conv1.weight', 'module.layer1.0.bn1.weight', 'module.layer1.0.bn1.bias', 'module.layer1.0.bn1.running_mean', 'module.layer1.0.bn1.running_var', 'module.layer1.0.bn1.num_batches_tracked', 'module.layer1.0.conv2.weight', 'module.layer1.0.bn2.weight', 'module.layer1.0.bn2.bias', 'module.layer1.0.bn2.running_mean', 'module.layer1.0.bn2.running_var', 'module.layer1.0.bn2.num_batches_tracked', 'module.layer1.0.conv3.weight', 'module.layer1.0.bn3.weight', 'module.layer1.0.bn3.bias', 'module.layer1.0.bn3.running_mean', 'module.layer1.0.bn3.running_var', 'module.layer1.0.bn3.num_batches_tracked', 'module.layer1.0.downsample.0.weight', 'module.layer1.0.downsample.1.weight', 'module.layer1.0.downsample.1.bias', 'module.layer1.0.downsample.1.running_mean', 'module.layer1.0.downsample.1.running_var', 'module.layer1.0.downsample.1.num_batches_tracked', 'module.layer1.1.conv1.weight', 'module.layer1.1.bn1.weight', 'module.layer1.1.bn1.bias', 'module.layer1.1.bn1.running_mean', 'module.layer1.1.bn1.running_var', 'module.layer1.1.bn1.num_batches_tracked', 'module.layer1.1.conv2.weight', 'module.layer1.1.bn2.weight', 'module.layer1.1.bn2.bias', 'module.layer1.1.bn2.running_mean', 'module.layer1.1.bn2.running_var', 'module.layer1.1.bn2.num_batches_tracked', 'module.layer1.1.conv3.weight', 'module.layer1.1.bn3.weight', 'module.layer1.1.bn3.bias', 'module.layer1.1.bn3.running_mean', 'module.layer1.1.bn3.running_var', 'module.layer1.1.bn3.num_batches_tracked', 'module.layer1.2.conv1.weight', 'module.layer1.2.bn1.weight', 'module.layer1.2.bn1.bias', 'module.layer1.2.bn1.running_mean', 'module.layer1.2.bn1.running_var', 'module.layer1.2.bn1.num_batches_tracked', 'module.layer1.2.conv2.weight', 'module.layer1.2.bn2.weight', 'module.layer1.2.bn2.bias', 'module.layer1.2.bn2.running_mean', 'module.layer1.2.bn2.running_var', 'module.layer1.2.bn2.num_batches_tracked', 'module.layer1.2.conv3.weight', 'module.layer1.2.bn3.weight', 'module.layer1.2.bn3.bias', 'module.layer1.2.bn3.running_mean', 'module.layer1.2.bn3.running_var', 'module.layer1.2.bn3.num_batches_tracked', 'module.layer2.0.conv1.weight', 'module.layer2.0.bn1.weight', 'module.layer2.0.bn1.bias', 'module.layer2.0.bn1.running_mean', 'module.layer2.0.bn1.running_var', 'module.layer2.0.bn1.num_batches_tracked', 'module.layer2.0.conv2.weight', 'module.layer2.0.bn2.weight', 'module.layer2.0.bn2.bias', 'module.layer2.0.bn2.running_mean', 'module.layer2.0.bn2.running_var', 'module.layer2.0.bn2.num_batches_tracked', 'module.layer2.0.conv3.weight', 'module.layer2.0.bn3.weight', 'module.layer2.0.bn3.bias', 'module.layer2.0.bn3.running_mean', 'module.layer2.0.bn3.running_var', 'module.layer2.0.bn3.num_batches_tracked', 'module.layer2.0.downsample.0.weight', 'module.layer2.0.downsample.1.weight', 'module.layer2.0.downsample.1.bias', 'module.layer2.0.downsample.1.running_mean', 'module.layer2.0.downsample.1.running_var', 'module.layer2.0.downsample.1.num_batches_tracked', 'module.layer2.1.conv1.weight', 'module.layer2.1.bn1.weight', 'module.layer2.1.bn1.bias', 'module.layer2.1.bn1.running_mean', 'module.layer2.1.bn1.running_var', 'module.layer2.1.bn1.num_batches_tracked', 'module.layer2.1.conv2.weight', 'module.layer2.1.bn2.weight', 'module.layer2.1.bn2.bias', 'module.layer2.1.bn2.running_mean', 'module.layer2.1.bn2.running_var', 'module.layer2.1.bn2.num_batches_tracked', 'module.layer2.1.conv3.weight', 'module.layer2.1.bn3.weight', 'module.layer2.1.bn3.bias', 'module.layer2.1.bn3.running_mean', 'module.layer2.1.bn3.running_var', 'module.layer2.1.bn3.num_batches_tracked', 'module.layer2.2.conv1.weight', 'module.layer2.2.bn1.weight', 'module.layer2.2.bn1.bias', 'module.layer2.2.bn1.running_mean', 'module.layer2.2.bn1.running_var', 'module.layer2.2.bn1.num_batches_tracked', 'module.layer2.2.conv2.weight', 'module.layer2.2.bn2.weight', 'module.layer2.2.bn2.bias', 'module.layer2.2.bn2.running_mean', 'module.layer2.2.bn2.running_var', 'module.layer2.2.bn2.num_batches_tracked', 'module.layer2.2.conv3.weight', 'module.layer2.2.bn3.weight', 'module.layer2.2.bn3.bias', 'module.layer2.2.bn3.running_mean', 'module.layer2.2.bn3.running_var', 'module.layer2.2.bn3.num_batches_tracked', 'module.layer2.3.conv1.weight', 'module.layer2.3.bn1.weight', 'module.layer2.3.bn1.bias', 'module.layer2.3.bn1.running_mean', 'module.layer2.3.bn1.running_var', 'module.layer2.3.bn1.num_batches_tracked', 'module.layer2.3.conv2.weight', 'module.layer2.3.bn2.weight', 'module.layer2.3.bn2.bias', 'module.layer2.3.bn2.running_mean', 'module.layer2.3.bn2.running_var', 'module.layer2.3.bn2.num_batches_tracked', 'module.layer2.3.conv3.weight', 'module.layer2.3.bn3.weight', 'module.layer2.3.bn3.bias', 'module.layer2.3.bn3.running_mean', 'module.layer2.3.bn3.running_var', 'module.layer2.3.bn3.num_batches_tracked', 'module.layer3.0.conv1.weight', 'module.layer3.0.bn1.weight', 'module.layer3.0.bn1.bias', 'module.layer3.0.bn1.running_mean', 'module.layer3.0.bn1.running_var', 'module.layer3.0.bn1.num_batches_tracked', 'module.layer3.0.conv2.weight', 'module.layer3.0.bn2.weight', 'module.layer3.0.bn2.bias', 'module.layer3.0.bn2.running_mean', 'module.layer3.0.bn2.running_var', 'module.layer3.0.bn2.num_batches_tracked', 'module.layer3.0.conv3.weight', 'module.layer3.0.bn3.weight', 'module.layer3.0.bn3.bias', 'module.layer3.0.bn3.running_mean', 'module.layer3.0.bn3.running_var', 'module.layer3.0.bn3.num_batches_tracked', 'module.layer3.0.downsample.0.weight', 'module.layer3.0.downsample.1.weight', 'module.layer3.0.downsample.1.bias', 'module.layer3.0.downsample.1.running_mean', 'module.layer3.0.downsample.1.running_var', 'module.layer3.0.downsample.1.num_batches_tracked', 'module.layer3.1.conv1.weight', 'module.layer3.1.bn1.weight', 'module.layer3.1.bn1.bias', 'module.layer3.1.bn1.running_mean', 'module.layer3.1.bn1.running_var', 'module.layer3.1.bn1.num_batches_tracked', 'module.layer3.1.conv2.weight', 'module.layer3.1.bn2.weight', 'module.layer3.1.bn2.bias', 'module.layer3.1.bn2.running_mean', 'module.layer3.1.bn2.running_var', 'module.layer3.1.bn2.num_batches_tracked', 'module.layer3.1.conv3.weight', 'module.layer3.1.bn3.weight', 'module.layer3.1.bn3.bias', 'module.layer3.1.bn3.running_mean', 'module.layer3.1.bn3.running_var', 'module.layer3.1.bn3.num_batches_tracked', 'module.layer3.2.conv1.weight', 'module.layer3.2.bn1.weight', 'module.layer3.2.bn1.bias', 'module.layer3.2.bn1.running_mean', 'module.layer3.2.bn1.running_var', 'module.layer3.2.bn1.num_batches_tracked', 'module.layer3.2.conv2.weight', 'module.layer3.2.bn2.weight', 'module.layer3.2.bn2.bias', 'module.layer3.2.bn2.running_mean', 'module.layer3.2.bn2.running_var', 'module.layer3.2.bn2.num_batches_tracked', 'module.layer3.2.conv3.weight', 'module.layer3.2.bn3.weight', 'module.layer3.2.bn3.bias', 'module.layer3.2.bn3.running_mean', 'module.layer3.2.bn3.running_var', 'module.layer3.2.bn3.num_batches_tracked', 'module.layer3.3.conv1.weight', 'module.layer3.3.bn1.weight', 'module.layer3.3.bn1.bias', 'module.layer3.3.bn1.running_mean', 'module.layer3.3.bn1.running_var', 'module.layer3.3.bn1.num_batches_tracked', 'module.layer3.3.conv2.weight', 'module.layer3.3.bn2.weight', 'module.layer3.3.bn2.bias', 'module.layer3.3.bn2.running_mean', 'module.layer3.3.bn2.running_var', 'module.layer3.3.bn2.num_batches_tracked', 'module.layer3.3.conv3.weight', 'module.layer3.3.bn3.weight', 'module.layer3.3.bn3.bias', 'module.layer3.3.bn3.running_mean', 'module.layer3.3.bn3.running_var', 'module.layer3.3.bn3.num_batches_tracked', 'module.layer3.4.conv1.weight', 'module.layer3.4.bn1.weight', 'module.layer3.4.bn1.bias', 'module.layer3.4.bn1.running_mean', 'module.layer3.4.bn1.running_var', 'module.layer3.4.bn1.num_batches_tracked', 'module.layer3.4.conv2.weight', 'module.layer3.4.bn2.weight', 'module.layer3.4.bn2.bias', 'module.layer3.4.bn2.running_mean', 'module.layer3.4.bn2.running_var', 'module.layer3.4.bn2.num_batches_tracked', 'module.layer3.4.conv3.weight', 'module.layer3.4.bn3.weight', 'module.layer3.4.bn3.bias', 'module.layer3.4.bn3.running_mean', 'module.layer3.4.bn3.running_var', 'module.layer3.4.bn3.num_batches_tracked', 'module.layer3.5.conv1.weight', 'module.layer3.5.bn1.weight', 'module.layer3.5.bn1.bias', 'module.layer3.5.bn1.running_mean', 'module.layer3.5.bn1.running_var', 'module.layer3.5.bn1.num_batches_tracked', 'module.layer3.5.conv2.weight', 'module.layer3.5.bn2.weight', 'module.layer3.5.bn2.bias', 'module.layer3.5.bn2.running_mean', 'module.layer3.5.bn2.running_var', 'module.layer3.5.bn2.num_batches_tracked', 'module.layer3.5.conv3.weight', 'module.layer3.5.bn3.weight', 'module.layer3.5.bn3.bias', 'module.layer3.5.bn3.running_mean', 'module.layer3.5.bn3.running_var', 'module.layer3.5.bn3.num_batches_tracked', 'module.layer4.0.conv1.weight', 'module.layer4.0.bn1.weight', 'module.layer4.0.bn1.bias', 'module.layer4.0.bn1.running_mean', 'module.layer4.0.bn1.running_var', 'module.layer4.0.bn1.num_batches_tracked', 'module.layer4.0.conv2.weight', 'module.layer4.0.bn2.weight', 'module.layer4.0.bn2.bias', 'module.layer4.0.bn2.running_mean', 'module.layer4.0.bn2.running_var', 'module.layer4.0.bn2.num_batches_tracked', 'module.layer4.0.conv3.weight', 'module.layer4.0.bn3.weight', 'module.layer4.0.bn3.bias', 'module.layer4.0.bn3.running_mean', 'module.layer4.0.bn3.running_var', 'module.layer4.0.bn3.num_batches_tracked', 'module.layer4.0.downsample.0.weight', 'module.layer4.0.downsample.1.weight', 'module.layer4.0.downsample.1.bias', 'module.layer4.0.downsample.1.running_mean', 'module.layer4.0.downsample.1.running_var', 'module.layer4.0.downsample.1.num_batches_tracked', 'module.layer4.1.conv1.weight', 'module.layer4.1.bn1.weight', 'module.layer4.1.bn1.bias', 'module.layer4.1.bn1.running_mean', 'module.layer4.1.bn1.running_var', 'module.layer4.1.bn1.num_batches_tracked', 'module.layer4.1.conv2.weight', 'module.layer4.1.bn2.weight', 'module.layer4.1.bn2.bias', 'module.layer4.1.bn2.running_mean', 'module.layer4.1.bn2.running_var', 'module.layer4.1.bn2.num_batches_tracked', 'module.layer4.1.conv3.weight', 'module.layer4.1.bn3.weight', 'module.layer4.1.bn3.bias', 'module.layer4.1.bn3.running_mean', 'module.layer4.1.bn3.running_var', 'module.layer4.1.bn3.num_batches_tracked', 'module.layer4.2.conv1.weight', 'module.layer4.2.bn1.weight', 'module.layer4.2.bn1.bias', 'module.layer4.2.bn1.running_mean', 'module.layer4.2.bn1.running_var', 'module.layer4.2.bn1.num_batches_tracked', 'module.layer4.2.conv2.weight', 'module.layer4.2.bn2.weight', 'module.layer4.2.bn2.bias', 'module.layer4.2.bn2.running_mean', 'module.layer4.2.bn2.running_var', 'module.layer4.2.bn2.num_batches_tracked', 'module.layer4.2.conv3.weight', 'module.layer4.2.bn3.weight', 'module.layer4.2.bn3.bias', 'module.layer4.2.bn3.running_mean', 'module.layer4.2.bn3.running_var', 'module.layer4.2.bn3.num_batches_tracked']\n",
            "TOTAL: 318\n",
            "======\n",
            "Pretrained weights found (TOTAL: 318):\n",
            "dict_keys(['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.bn1.running_mean', 'module.bn1.running_var', 'module.bn1.num_batches_tracked', 'module.layer1.0.conv1.weight', 'module.layer1.0.bn1.weight', 'module.layer1.0.bn1.bias', 'module.layer1.0.bn1.running_mean', 'module.layer1.0.bn1.running_var', 'module.layer1.0.bn1.num_batches_tracked', 'module.layer1.0.conv2.weight', 'module.layer1.0.bn2.weight', 'module.layer1.0.bn2.bias', 'module.layer1.0.bn2.running_mean', 'module.layer1.0.bn2.running_var', 'module.layer1.0.bn2.num_batches_tracked', 'module.layer1.0.conv3.weight', 'module.layer1.0.bn3.weight', 'module.layer1.0.bn3.bias', 'module.layer1.0.bn3.running_mean', 'module.layer1.0.bn3.running_var', 'module.layer1.0.bn3.num_batches_tracked', 'module.layer1.0.downsample.0.weight', 'module.layer1.0.downsample.1.weight', 'module.layer1.0.downsample.1.bias', 'module.layer1.0.downsample.1.running_mean', 'module.layer1.0.downsample.1.running_var', 'module.layer1.0.downsample.1.num_batches_tracked', 'module.layer1.1.conv1.weight', 'module.layer1.1.bn1.weight', 'module.layer1.1.bn1.bias', 'module.layer1.1.bn1.running_mean', 'module.layer1.1.bn1.running_var', 'module.layer1.1.bn1.num_batches_tracked', 'module.layer1.1.conv2.weight', 'module.layer1.1.bn2.weight', 'module.layer1.1.bn2.bias', 'module.layer1.1.bn2.running_mean', 'module.layer1.1.bn2.running_var', 'module.layer1.1.bn2.num_batches_tracked', 'module.layer1.1.conv3.weight', 'module.layer1.1.bn3.weight', 'module.layer1.1.bn3.bias', 'module.layer1.1.bn3.running_mean', 'module.layer1.1.bn3.running_var', 'module.layer1.1.bn3.num_batches_tracked', 'module.layer1.2.conv1.weight', 'module.layer1.2.bn1.weight', 'module.layer1.2.bn1.bias', 'module.layer1.2.bn1.running_mean', 'module.layer1.2.bn1.running_var', 'module.layer1.2.bn1.num_batches_tracked', 'module.layer1.2.conv2.weight', 'module.layer1.2.bn2.weight', 'module.layer1.2.bn2.bias', 'module.layer1.2.bn2.running_mean', 'module.layer1.2.bn2.running_var', 'module.layer1.2.bn2.num_batches_tracked', 'module.layer1.2.conv3.weight', 'module.layer1.2.bn3.weight', 'module.layer1.2.bn3.bias', 'module.layer1.2.bn3.running_mean', 'module.layer1.2.bn3.running_var', 'module.layer1.2.bn3.num_batches_tracked', 'module.layer2.0.conv1.weight', 'module.layer2.0.bn1.weight', 'module.layer2.0.bn1.bias', 'module.layer2.0.bn1.running_mean', 'module.layer2.0.bn1.running_var', 'module.layer2.0.bn1.num_batches_tracked', 'module.layer2.0.conv2.weight', 'module.layer2.0.bn2.weight', 'module.layer2.0.bn2.bias', 'module.layer2.0.bn2.running_mean', 'module.layer2.0.bn2.running_var', 'module.layer2.0.bn2.num_batches_tracked', 'module.layer2.0.conv3.weight', 'module.layer2.0.bn3.weight', 'module.layer2.0.bn3.bias', 'module.layer2.0.bn3.running_mean', 'module.layer2.0.bn3.running_var', 'module.layer2.0.bn3.num_batches_tracked', 'module.layer2.0.downsample.0.weight', 'module.layer2.0.downsample.1.weight', 'module.layer2.0.downsample.1.bias', 'module.layer2.0.downsample.1.running_mean', 'module.layer2.0.downsample.1.running_var', 'module.layer2.0.downsample.1.num_batches_tracked', 'module.layer2.1.conv1.weight', 'module.layer2.1.bn1.weight', 'module.layer2.1.bn1.bias', 'module.layer2.1.bn1.running_mean', 'module.layer2.1.bn1.running_var', 'module.layer2.1.bn1.num_batches_tracked', 'module.layer2.1.conv2.weight', 'module.layer2.1.bn2.weight', 'module.layer2.1.bn2.bias', 'module.layer2.1.bn2.running_mean', 'module.layer2.1.bn2.running_var', 'module.layer2.1.bn2.num_batches_tracked', 'module.layer2.1.conv3.weight', 'module.layer2.1.bn3.weight', 'module.layer2.1.bn3.bias', 'module.layer2.1.bn3.running_mean', 'module.layer2.1.bn3.running_var', 'module.layer2.1.bn3.num_batches_tracked', 'module.layer2.2.conv1.weight', 'module.layer2.2.bn1.weight', 'module.layer2.2.bn1.bias', 'module.layer2.2.bn1.running_mean', 'module.layer2.2.bn1.running_var', 'module.layer2.2.bn1.num_batches_tracked', 'module.layer2.2.conv2.weight', 'module.layer2.2.bn2.weight', 'module.layer2.2.bn2.bias', 'module.layer2.2.bn2.running_mean', 'module.layer2.2.bn2.running_var', 'module.layer2.2.bn2.num_batches_tracked', 'module.layer2.2.conv3.weight', 'module.layer2.2.bn3.weight', 'module.layer2.2.bn3.bias', 'module.layer2.2.bn3.running_mean', 'module.layer2.2.bn3.running_var', 'module.layer2.2.bn3.num_batches_tracked', 'module.layer2.3.conv1.weight', 'module.layer2.3.bn1.weight', 'module.layer2.3.bn1.bias', 'module.layer2.3.bn1.running_mean', 'module.layer2.3.bn1.running_var', 'module.layer2.3.bn1.num_batches_tracked', 'module.layer2.3.conv2.weight', 'module.layer2.3.bn2.weight', 'module.layer2.3.bn2.bias', 'module.layer2.3.bn2.running_mean', 'module.layer2.3.bn2.running_var', 'module.layer2.3.bn2.num_batches_tracked', 'module.layer2.3.conv3.weight', 'module.layer2.3.bn3.weight', 'module.layer2.3.bn3.bias', 'module.layer2.3.bn3.running_mean', 'module.layer2.3.bn3.running_var', 'module.layer2.3.bn3.num_batches_tracked', 'module.layer3.0.conv1.weight', 'module.layer3.0.bn1.weight', 'module.layer3.0.bn1.bias', 'module.layer3.0.bn1.running_mean', 'module.layer3.0.bn1.running_var', 'module.layer3.0.bn1.num_batches_tracked', 'module.layer3.0.conv2.weight', 'module.layer3.0.bn2.weight', 'module.layer3.0.bn2.bias', 'module.layer3.0.bn2.running_mean', 'module.layer3.0.bn2.running_var', 'module.layer3.0.bn2.num_batches_tracked', 'module.layer3.0.conv3.weight', 'module.layer3.0.bn3.weight', 'module.layer3.0.bn3.bias', 'module.layer3.0.bn3.running_mean', 'module.layer3.0.bn3.running_var', 'module.layer3.0.bn3.num_batches_tracked', 'module.layer3.0.downsample.0.weight', 'module.layer3.0.downsample.1.weight', 'module.layer3.0.downsample.1.bias', 'module.layer3.0.downsample.1.running_mean', 'module.layer3.0.downsample.1.running_var', 'module.layer3.0.downsample.1.num_batches_tracked', 'module.layer3.1.conv1.weight', 'module.layer3.1.bn1.weight', 'module.layer3.1.bn1.bias', 'module.layer3.1.bn1.running_mean', 'module.layer3.1.bn1.running_var', 'module.layer3.1.bn1.num_batches_tracked', 'module.layer3.1.conv2.weight', 'module.layer3.1.bn2.weight', 'module.layer3.1.bn2.bias', 'module.layer3.1.bn2.running_mean', 'module.layer3.1.bn2.running_var', 'module.layer3.1.bn2.num_batches_tracked', 'module.layer3.1.conv3.weight', 'module.layer3.1.bn3.weight', 'module.layer3.1.bn3.bias', 'module.layer3.1.bn3.running_mean', 'module.layer3.1.bn3.running_var', 'module.layer3.1.bn3.num_batches_tracked', 'module.layer3.2.conv1.weight', 'module.layer3.2.bn1.weight', 'module.layer3.2.bn1.bias', 'module.layer3.2.bn1.running_mean', 'module.layer3.2.bn1.running_var', 'module.layer3.2.bn1.num_batches_tracked', 'module.layer3.2.conv2.weight', 'module.layer3.2.bn2.weight', 'module.layer3.2.bn2.bias', 'module.layer3.2.bn2.running_mean', 'module.layer3.2.bn2.running_var', 'module.layer3.2.bn2.num_batches_tracked', 'module.layer3.2.conv3.weight', 'module.layer3.2.bn3.weight', 'module.layer3.2.bn3.bias', 'module.layer3.2.bn3.running_mean', 'module.layer3.2.bn3.running_var', 'module.layer3.2.bn3.num_batches_tracked', 'module.layer3.3.conv1.weight', 'module.layer3.3.bn1.weight', 'module.layer3.3.bn1.bias', 'module.layer3.3.bn1.running_mean', 'module.layer3.3.bn1.running_var', 'module.layer3.3.bn1.num_batches_tracked', 'module.layer3.3.conv2.weight', 'module.layer3.3.bn2.weight', 'module.layer3.3.bn2.bias', 'module.layer3.3.bn2.running_mean', 'module.layer3.3.bn2.running_var', 'module.layer3.3.bn2.num_batches_tracked', 'module.layer3.3.conv3.weight', 'module.layer3.3.bn3.weight', 'module.layer3.3.bn3.bias', 'module.layer3.3.bn3.running_mean', 'module.layer3.3.bn3.running_var', 'module.layer3.3.bn3.num_batches_tracked', 'module.layer3.4.conv1.weight', 'module.layer3.4.bn1.weight', 'module.layer3.4.bn1.bias', 'module.layer3.4.bn1.running_mean', 'module.layer3.4.bn1.running_var', 'module.layer3.4.bn1.num_batches_tracked', 'module.layer3.4.conv2.weight', 'module.layer3.4.bn2.weight', 'module.layer3.4.bn2.bias', 'module.layer3.4.bn2.running_mean', 'module.layer3.4.bn2.running_var', 'module.layer3.4.bn2.num_batches_tracked', 'module.layer3.4.conv3.weight', 'module.layer3.4.bn3.weight', 'module.layer3.4.bn3.bias', 'module.layer3.4.bn3.running_mean', 'module.layer3.4.bn3.running_var', 'module.layer3.4.bn3.num_batches_tracked', 'module.layer3.5.conv1.weight', 'module.layer3.5.bn1.weight', 'module.layer3.5.bn1.bias', 'module.layer3.5.bn1.running_mean', 'module.layer3.5.bn1.running_var', 'module.layer3.5.bn1.num_batches_tracked', 'module.layer3.5.conv2.weight', 'module.layer3.5.bn2.weight', 'module.layer3.5.bn2.bias', 'module.layer3.5.bn2.running_mean', 'module.layer3.5.bn2.running_var', 'module.layer3.5.bn2.num_batches_tracked', 'module.layer3.5.conv3.weight', 'module.layer3.5.bn3.weight', 'module.layer3.5.bn3.bias', 'module.layer3.5.bn3.running_mean', 'module.layer3.5.bn3.running_var', 'module.layer3.5.bn3.num_batches_tracked', 'module.layer4.0.conv1.weight', 'module.layer4.0.bn1.weight', 'module.layer4.0.bn1.bias', 'module.layer4.0.bn1.running_mean', 'module.layer4.0.bn1.running_var', 'module.layer4.0.bn1.num_batches_tracked', 'module.layer4.0.conv2.weight', 'module.layer4.0.bn2.weight', 'module.layer4.0.bn2.bias', 'module.layer4.0.bn2.running_mean', 'module.layer4.0.bn2.running_var', 'module.layer4.0.bn2.num_batches_tracked', 'module.layer4.0.conv3.weight', 'module.layer4.0.bn3.weight', 'module.layer4.0.bn3.bias', 'module.layer4.0.bn3.running_mean', 'module.layer4.0.bn3.running_var', 'module.layer4.0.bn3.num_batches_tracked', 'module.layer4.0.downsample.0.weight', 'module.layer4.0.downsample.1.weight', 'module.layer4.0.downsample.1.bias', 'module.layer4.0.downsample.1.running_mean', 'module.layer4.0.downsample.1.running_var', 'module.layer4.0.downsample.1.num_batches_tracked', 'module.layer4.1.conv1.weight', 'module.layer4.1.bn1.weight', 'module.layer4.1.bn1.bias', 'module.layer4.1.bn1.running_mean', 'module.layer4.1.bn1.running_var', 'module.layer4.1.bn1.num_batches_tracked', 'module.layer4.1.conv2.weight', 'module.layer4.1.bn2.weight', 'module.layer4.1.bn2.bias', 'module.layer4.1.bn2.running_mean', 'module.layer4.1.bn2.running_var', 'module.layer4.1.bn2.num_batches_tracked', 'module.layer4.1.conv3.weight', 'module.layer4.1.bn3.weight', 'module.layer4.1.bn3.bias', 'module.layer4.1.bn3.running_mean', 'module.layer4.1.bn3.running_var', 'module.layer4.1.bn3.num_batches_tracked', 'module.layer4.2.conv1.weight', 'module.layer4.2.bn1.weight', 'module.layer4.2.bn1.bias', 'module.layer4.2.bn1.running_mean', 'module.layer4.2.bn1.running_var', 'module.layer4.2.bn1.num_batches_tracked', 'module.layer4.2.conv2.weight', 'module.layer4.2.bn2.weight', 'module.layer4.2.bn2.bias', 'module.layer4.2.bn2.running_mean', 'module.layer4.2.bn2.running_var', 'module.layer4.2.bn2.num_batches_tracked', 'module.layer4.2.conv3.weight', 'module.layer4.2.bn3.weight', 'module.layer4.2.bn3.bias', 'module.layer4.2.bn3.running_mean', 'module.layer4.2.bn3.running_var', 'module.layer4.2.bn3.num_batches_tracked'])\n",
            "\n",
            "Using steps for training.\n",
            "Initializing model optimizer.\n",
            "===> Using coslr eta_min=0.0\n",
            "Loading Softmax Loss.\n",
            "===> Saving cfg parameters to:  ./data/cfg.yaml\n",
            "Phase: train\n",
            "Do shuffle??? ---  True\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Epoch: [1/90] Step:     0  Minibatch_loss_performance: 6.902 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    10  Minibatch_loss_performance: 6.946 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    20  Minibatch_loss_performance: 6.947 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    30  Minibatch_loss_performance: 6.956 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    40  Minibatch_loss_performance: 6.914 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    50  Minibatch_loss_performance: 6.930 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    60  Minibatch_loss_performance: 6.945 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    70  Minibatch_loss_performance: 6.908 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    80  Minibatch_loss_performance: 6.933 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:    90  Minibatch_loss_performance: 6.947 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   100  Minibatch_loss_performance: 6.895 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   110  Minibatch_loss_performance: 6.927 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   120  Minibatch_loss_performance: 6.947 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   130  Minibatch_loss_performance: 6.901 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   140  Minibatch_loss_performance: 6.863 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   150  Minibatch_loss_performance: 6.927 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   160  Minibatch_loss_performance: 6.893 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   170  Minibatch_loss_performance: 6.915 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   180  Minibatch_loss_performance: 6.935 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   190  Minibatch_loss_performance: 6.898 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   200  Minibatch_loss_performance: 6.888 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   210  Minibatch_loss_performance: 6.919 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   220  Minibatch_loss_performance: 6.892 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   230  Minibatch_loss_performance: 6.893 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   240  Minibatch_loss_performance: 6.882 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   250  Minibatch_loss_performance: 6.846 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   260  Minibatch_loss_performance: 6.886 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   270  Minibatch_loss_performance: 6.918 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   280  Minibatch_loss_performance: 6.899 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [1/90] Step:   290  Minibatch_loss_performance: 6.864 Minibatch_accuracy_micro: 0.000\n",
            "\n",
            " Training acc Top1: 0.000 \n",
            " Many_top1: 0.000 Median_top1: 0.000 Low_top1: 0.001 \n",
            "\n",
            "Phase: val\n",
            "100% 125/125 [08:01<00:00,  3.85s/it]\n",
            "\n",
            "\n",
            " Phase: val \n",
            "\n",
            " Evaluation_accuracy_micro_top1: 0.001 \n",
            " Averaged F-measure: 0.000 \n",
            " Many_shot_accuracy_top1: 0.000 Median_shot_accuracy_top1: 0.001 Low_shot_accuracy_top1: 0.002 \n",
            "\n",
            "===> Saving checkpoint\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch: [2/90] Step:     0  Minibatch_loss_performance: 6.826 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    10  Minibatch_loss_performance: 6.876 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    20  Minibatch_loss_performance: 6.921 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    30  Minibatch_loss_performance: 6.866 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    40  Minibatch_loss_performance: 6.952 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    50  Minibatch_loss_performance: 6.969 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    60  Minibatch_loss_performance: 6.837 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    70  Minibatch_loss_performance: 6.771 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    80  Minibatch_loss_performance: 6.933 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:    90  Minibatch_loss_performance: 6.822 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   100  Minibatch_loss_performance: 6.806 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   110  Minibatch_loss_performance: 6.955 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   120  Minibatch_loss_performance: 6.802 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   130  Minibatch_loss_performance: 6.870 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   140  Minibatch_loss_performance: 6.950 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   150  Minibatch_loss_performance: 6.811 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   160  Minibatch_loss_performance: 6.780 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   170  Minibatch_loss_performance: 6.882 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   180  Minibatch_loss_performance: 6.833 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   190  Minibatch_loss_performance: 6.727 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   200  Minibatch_loss_performance: 6.865 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   210  Minibatch_loss_performance: 6.751 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   220  Minibatch_loss_performance: 6.808 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   230  Minibatch_loss_performance: 6.783 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   240  Minibatch_loss_performance: 6.788 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   250  Minibatch_loss_performance: 6.697 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   260  Minibatch_loss_performance: 6.805 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   270  Minibatch_loss_performance: 6.916 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   280  Minibatch_loss_performance: 6.652 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [2/90] Step:   290  Minibatch_loss_performance: 6.869 Minibatch_accuracy_micro: 0.000\n",
            "\n",
            " Training acc Top1: 0.001 \n",
            " Many_top1: 0.000 Median_top1: 0.001 Low_top1: 0.001 \n",
            "\n",
            "Phase: val\n",
            "100% 125/125 [06:00<00:00,  2.88s/it]\n",
            "\n",
            "\n",
            " Phase: val \n",
            "\n",
            " Evaluation_accuracy_micro_top1: 0.002 \n",
            " Averaged F-measure: 0.000 \n",
            " Many_shot_accuracy_top1: 0.009 Median_shot_accuracy_top1: 0.001 Low_shot_accuracy_top1: 0.003 \n",
            "\n",
            "===> Saving checkpoint\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch: [3/90] Step:     0  Minibatch_loss_performance: 6.583 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    10  Minibatch_loss_performance: 6.685 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    20  Minibatch_loss_performance: 6.794 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    30  Minibatch_loss_performance: 6.874 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    40  Minibatch_loss_performance: 6.888 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    50  Minibatch_loss_performance: 6.826 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    60  Minibatch_loss_performance: 6.730 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    70  Minibatch_loss_performance: 6.886 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    80  Minibatch_loss_performance: 6.664 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:    90  Minibatch_loss_performance: 6.827 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   100  Minibatch_loss_performance: 6.794 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   110  Minibatch_loss_performance: 6.635 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   120  Minibatch_loss_performance: 6.852 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   130  Minibatch_loss_performance: 6.951 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   140  Minibatch_loss_performance: 6.648 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   150  Minibatch_loss_performance: 6.748 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   160  Minibatch_loss_performance: 6.786 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   170  Minibatch_loss_performance: 6.513 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   180  Minibatch_loss_performance: 6.654 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   190  Minibatch_loss_performance: 6.863 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   200  Minibatch_loss_performance: 6.628 Minibatch_accuracy_micro: 0.016\n",
            "Epoch: [3/90] Step:   210  Minibatch_loss_performance: 6.684 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   220  Minibatch_loss_performance: 6.740 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   230  Minibatch_loss_performance: 6.510 Minibatch_accuracy_micro: 0.023\n",
            "Epoch: [3/90] Step:   240  Minibatch_loss_performance: 6.663 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   250  Minibatch_loss_performance: 6.588 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   260  Minibatch_loss_performance: 6.633 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   270  Minibatch_loss_performance: 6.543 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   280  Minibatch_loss_performance: 6.585 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [3/90] Step:   290  Minibatch_loss_performance: 6.843 Minibatch_accuracy_micro: 0.000\n",
            "\n",
            " Training acc Top1: 0.002 \n",
            " Many_top1: 0.003 Median_top1: 0.000 Low_top1: 0.003 \n",
            "\n",
            "Phase: val\n",
            "100% 125/125 [05:58<00:00,  2.86s/it]\n",
            "\n",
            "\n",
            " Phase: val \n",
            "\n",
            " Evaluation_accuracy_micro_top1: 0.005 \n",
            " Averaged F-measure: 0.001 \n",
            " Many_shot_accuracy_top1: 0.001 Median_shot_accuracy_top1: 0.004 Low_shot_accuracy_top1: 0.008 \n",
            "\n",
            "===> Saving checkpoint\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch: [4/90] Step:     0  Minibatch_loss_performance: 6.510 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [4/90] Step:    10  Minibatch_loss_performance: 6.526 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    20  Minibatch_loss_performance: 6.681 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    30  Minibatch_loss_performance: 6.731 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    40  Minibatch_loss_performance: 6.629 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    50  Minibatch_loss_performance: 6.638 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [4/90] Step:    60  Minibatch_loss_performance: 6.899 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    70  Minibatch_loss_performance: 6.598 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    80  Minibatch_loss_performance: 6.564 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:    90  Minibatch_loss_performance: 6.651 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   100  Minibatch_loss_performance: 6.656 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   110  Minibatch_loss_performance: 6.636 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   120  Minibatch_loss_performance: 6.689 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   130  Minibatch_loss_performance: 6.508 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   140  Minibatch_loss_performance: 6.500 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   150  Minibatch_loss_performance: 6.495 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   160  Minibatch_loss_performance: 6.470 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   170  Minibatch_loss_performance: 6.526 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   180  Minibatch_loss_performance: 6.652 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   190  Minibatch_loss_performance: 6.427 Minibatch_accuracy_micro: 0.031\n",
            "Epoch: [4/90] Step:   200  Minibatch_loss_performance: 6.628 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   210  Minibatch_loss_performance: 6.718 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   220  Minibatch_loss_performance: 6.368 Minibatch_accuracy_micro: 0.023\n",
            "Epoch: [4/90] Step:   230  Minibatch_loss_performance: 6.437 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   240  Minibatch_loss_performance: 6.438 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   250  Minibatch_loss_performance: 6.515 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [4/90] Step:   260  Minibatch_loss_performance: 6.428 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   270  Minibatch_loss_performance: 6.718 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   280  Minibatch_loss_performance: 6.676 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [4/90] Step:   290  Minibatch_loss_performance: 6.373 Minibatch_accuracy_micro: 0.008\n",
            "\n",
            " Training acc Top1: 0.003 \n",
            " Many_top1: 0.000 Median_top1: 0.001 Low_top1: 0.006 \n",
            "\n",
            "Phase: val\n",
            "100% 125/125 [06:09<00:00,  2.96s/it]\n",
            "\n",
            "\n",
            " Phase: val \n",
            "\n",
            " Evaluation_accuracy_micro_top1: 0.007 \n",
            " Averaged F-measure: 0.002 \n",
            " Many_shot_accuracy_top1: 0.005 Median_shot_accuracy_top1: 0.003 Low_shot_accuracy_top1: 0.012 \n",
            "\n",
            "===> Saving checkpoint\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch: [5/90] Step:     0  Minibatch_loss_performance: 6.483 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    10  Minibatch_loss_performance: 6.322 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    20  Minibatch_loss_performance: 6.556 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    30  Minibatch_loss_performance: 6.600 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [5/90] Step:    40  Minibatch_loss_performance: 6.488 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    50  Minibatch_loss_performance: 6.682 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    60  Minibatch_loss_performance: 6.565 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    70  Minibatch_loss_performance: 6.593 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    80  Minibatch_loss_performance: 6.573 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:    90  Minibatch_loss_performance: 6.339 Minibatch_accuracy_micro: 0.023\n",
            "Epoch: [5/90] Step:   100  Minibatch_loss_performance: 6.602 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   110  Minibatch_loss_performance: 6.693 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   120  Minibatch_loss_performance: 6.275 Minibatch_accuracy_micro: 0.023\n",
            "Epoch: [5/90] Step:   130  Minibatch_loss_performance: 6.412 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   140  Minibatch_loss_performance: 6.577 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   150  Minibatch_loss_performance: 6.290 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   160  Minibatch_loss_performance: 6.433 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   170  Minibatch_loss_performance: 6.609 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   180  Minibatch_loss_performance: 6.341 Minibatch_accuracy_micro: 0.031\n",
            "Epoch: [5/90] Step:   190  Minibatch_loss_performance: 6.635 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   200  Minibatch_loss_performance: 6.661 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   210  Minibatch_loss_performance: 6.223 Minibatch_accuracy_micro: 0.016\n",
            "Epoch: [5/90] Step:   220  Minibatch_loss_performance: 6.172 Minibatch_accuracy_micro: 0.016\n",
            "Epoch: [5/90] Step:   230  Minibatch_loss_performance: 6.730 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   240  Minibatch_loss_performance: 6.247 Minibatch_accuracy_micro: 0.047\n",
            "Epoch: [5/90] Step:   250  Minibatch_loss_performance: 6.461 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   260  Minibatch_loss_performance: 6.623 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   270  Minibatch_loss_performance: 6.693 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   280  Minibatch_loss_performance: 6.264 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [5/90] Step:   290  Minibatch_loss_performance: 6.638 Minibatch_accuracy_micro: 0.000\n",
            "\n",
            " Training acc Top1: 0.005 \n",
            " Many_top1: 0.002 Median_top1: 0.002 Low_top1: 0.008 \n",
            "\n",
            "Phase: val\n",
            "100% 125/125 [06:01<00:00,  2.89s/it]\n",
            "\n",
            "\n",
            " Phase: val \n",
            "\n",
            " Evaluation_accuracy_micro_top1: 0.008 \n",
            " Averaged F-measure: 0.004 \n",
            " Many_shot_accuracy_top1: 0.006 Median_shot_accuracy_top1: 0.003 Low_shot_accuracy_top1: 0.014 \n",
            "\n",
            "===> Saving checkpoint\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch: [6/90] Step:     0  Minibatch_loss_performance: 6.332 Minibatch_accuracy_micro: 0.039\n",
            "Epoch: [6/90] Step:    10  Minibatch_loss_performance: 6.226 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    20  Minibatch_loss_performance: 6.216 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    30  Minibatch_loss_performance: 6.562 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    40  Minibatch_loss_performance: 6.507 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    50  Minibatch_loss_performance: 6.324 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    60  Minibatch_loss_performance: 6.500 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    70  Minibatch_loss_performance: 6.685 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [6/90] Step:    80  Minibatch_loss_performance: 6.354 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:    90  Minibatch_loss_performance: 6.495 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   100  Minibatch_loss_performance: 6.484 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   110  Minibatch_loss_performance: 6.193 Minibatch_accuracy_micro: 0.031\n",
            "Epoch: [6/90] Step:   120  Minibatch_loss_performance: 6.356 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   130  Minibatch_loss_performance: 6.875 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   140  Minibatch_loss_performance: 6.191 Minibatch_accuracy_micro: 0.016\n",
            "Epoch: [6/90] Step:   150  Minibatch_loss_performance: 6.493 Minibatch_accuracy_micro: 0.016\n",
            "Epoch: [6/90] Step:   160  Minibatch_loss_performance: 6.642 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [6/90] Step:   170  Minibatch_loss_performance: 6.118 Minibatch_accuracy_micro: 0.031\n",
            "Epoch: [6/90] Step:   180  Minibatch_loss_performance: 6.576 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   190  Minibatch_loss_performance: 6.618 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   200  Minibatch_loss_performance: 6.134 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   210  Minibatch_loss_performance: 6.234 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [6/90] Step:   220  Minibatch_loss_performance: 6.398 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   230  Minibatch_loss_performance: 6.057 Minibatch_accuracy_micro: 0.039\n",
            "Epoch: [6/90] Step:   240  Minibatch_loss_performance: 6.199 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [6/90] Step:   250  Minibatch_loss_performance: 6.506 Minibatch_accuracy_micro: 0.016\n",
            "Epoch: [6/90] Step:   260  Minibatch_loss_performance: 6.107 Minibatch_accuracy_micro: 0.008\n",
            "Epoch: [6/90] Step:   270  Minibatch_loss_performance: 6.200 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   280  Minibatch_loss_performance: 6.426 Minibatch_accuracy_micro: 0.000\n",
            "Epoch: [6/90] Step:   290  Minibatch_loss_performance: 6.744 Minibatch_accuracy_micro: 0.000\n",
            "\n",
            " Training acc Top1: 0.007 \n",
            " Many_top1: 0.003 Median_top1: 0.002 Low_top1: 0.013 \n",
            "\n",
            "Phase: val\n",
            " 96% 120/125 [05:46<00:14,  2.89s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/imagenet_inat/main.py\", line 97, in <module>\n",
            "    training_model.train()\n",
            "  File \"/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/imagenet_inat/run_networks.py\", line 328, in train\n",
            "    rsls_eval = self.eval(phase='val')\n",
            "  File \"/content/drive/MyDrive/BDAProject/imbalanced-semi-self-master/imagenet_inat/run_networks.py\", line 444, in eval\n",
            "    for inputs, labels, paths in tqdm(self.data[phase]):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tqdm/std.py\", line 1195, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1359, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1325, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1163, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
            "    if not self._poll(timeout):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
            "    return self._poll(timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
            "    r = wait([self], timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"# python -m imagenet_inat.main --cfg <path_to_ssp_config> --model_dir <path_to_ssp_model>"
      ],
      "metadata": {
        "id": "1ANNfB03VXR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}